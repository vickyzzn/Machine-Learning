{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Ingestion and Cleaning\n",
    "\n",
    "In the Phase 2 of the Case Study, we will carry out the following steps:\n",
    "  - Ingest raw downloaded data\n",
    "  - Output a combined dataset ready for analysis and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sys import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that you'll be using while reading the raw files\n",
    "def is_integer(x):\n",
    "    '''\n",
    "    This function returns True if x is an integer, and False otherwise\n",
    "    '''\n",
    "    try:\n",
    "        return (int(x) == float(x))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories that contain the files downloaded\n",
    "dir_cs = os.getcwd() + \"/1805_download\" # path to the directory where all the *.csv.zip files are located\n",
    "\n",
    "# Define the output path for the pickle\n",
    "pickle_file =  \"./clean_data.pickle\" # path to save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns we'll be keeping from the dataset\n",
    "cols_to_pick = ['id', 'loan_amnt', 'funded_amnt', 'term', 'int_rate', 'installment', 'grade', 'emp_length',\n",
    "'home_ownership', 'annual_inc', 'verification_status', 'issue_d', 'loan_status',\n",
    "'purpose', 'dti', 'delinq_2yrs', 'earliest_cr_line', 'open_acc', 'pub_rec',\n",
    "'fico_range_high', 'fico_range_low', 'revol_bal', 'revol_util', 'total_pymnt',\n",
    "'last_pymnt_d', 'recoveries'] # list of features to use for this study as indicated in the handout\n",
    "\n",
    "# Identify the type of each of these column based on your CS-Phase 1 response\n",
    "float_cols = ['loan_amnt', 'funded_amnt', 'installment','annual_inc','dti','delinq_2yrs','open_acc','pub_rec',\n",
    "              'fico_range_high', 'fico_range_low','revol_bal','total_pymnt','recoveries']\n",
    "cat_cols = ['term','grade', 'emp_length','home_ownership','verification_status', 'loan_status', 'purpose', 'earliesr_cr_line'] # categorical features\n",
    "perc_cols = ['int_rate', 'revol_util']\n",
    "date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d']\n",
    "\n",
    "# Ensure that we have types for every column\n",
    "assert set(cols_to_pick) - set(float_cols) - set(cat_cols) - set(perc_cols) - set(date_cols) == set([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the columns selected will not be used directly in the model,\n",
    "# but will be used to generate other features.\n",
    "#\n",
    "# Create variables specifying the features that will be used\n",
    "\n",
    "# All categorical columns other than \"loan_status\" will be used as\n",
    "# discrete features\n",
    "\n",
    "discrete_features = list(set(cat_cols) - set([\"loan_status\"]))\n",
    "\n",
    "# All numeric columns will be used as continuous features\n",
    "continuous_features = list(float_cols + perc_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(dir_cs[0] != \"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "Ingest the data files from both sets, perform consistency checks, and prepare one single file for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_files(directory):\n",
    "    '''\n",
    "    This function will ingest every file in the specified directory\n",
    "    into a pandas dataframe. It will return a dictionary containing\n",
    "    these dataframes, keyed by the file name.\n",
    "    \n",
    "    We assume the directory contains files directly downloaded from\n",
    "    the link given in the handout, and *only* those files. Thus, we \n",
    "    assume the files are zipped (pd.read_csv can read zipped files) \n",
    "    and we assume the first line in each file needs to be skipped.\n",
    "    \n",
    "    Note that each file will be read *without* formatting\n",
    "    '''\n",
    "    \n",
    "    # If the directory has no trailing slash, add one\n",
    "    if directory[-1] != \"/\":\n",
    "        directory =  directory + \"/\"\n",
    "    all_files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".zip\"):\n",
    "            all_files.append(file)\n",
    "    print(all_files)\n",
    "    output = {}\n",
    "    \n",
    "    print(\"Directory \" + directory + \" has \" + str(len(all_files)) + \" files:\")\n",
    "    for i in all_files:\n",
    "        print(\"    Reading file \" + i)\n",
    "        output[i] = pd.read_csv(directory + i, dtype='str', header=1) # read each with dtype='str' and skip_rows =1\n",
    "        \n",
    "        # Some of the files have \"summary\" lines that, for example\n",
    "        # read \"Total number of loans number in Policy 1: .....\"\n",
    "        # To remove those lines, find any lines with non-integer IDs\n",
    "        # and remove them\n",
    "        l = output[i]['id'].tolist()\n",
    "        invalid_rows = [x for x, y in enumerate(l) if not is_integer(y)]# mask rows that have non-integer IDs. Use is_integer method\n",
    "        if len(invalid_rows) != 0:\n",
    "            print(\"Found \" + str(len(invalid_rows)) + \" invalid rows which were removed\")\n",
    "            output[i].drop(output[i].index[invalid_rows])\n",
    "    \n",
    "    return output # return dictionary of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LoanStats_securev1_2016Q3.csv.zip', 'LoanStats_securev1_2016Q2.csv.zip', 'LoanStats3a_securev1.csv.zip', 'LoanStats_securev1_2017Q1.csv.zip', 'LoanStats3d_securev1.csv.zip', 'LoanStats_securev1_2017Q3.csv.zip', 'LoanStats_securev1_2017Q2.csv.zip', 'LoanStats_securev1_2016Q1.csv.zip', 'LoanStats_securev1_2016Q4.csv.zip', 'LoanStats3c_securev1.csv.zip', 'LoanStats_securev1_2017Q4.csv.zip', 'LoanStats3b_securev1.csv.zip']\n",
      "Directory /Users/vicky/Downloads/1805_download/ has 12 files:\n",
      "    Reading file LoanStats_securev1_2016Q3.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats_securev1_2016Q2.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats3a_securev1.csv.zip\n",
      "Found 3 invalid rows which were removed\n",
      "    Reading file LoanStats_securev1_2017Q1.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats3d_securev1.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats_securev1_2017Q3.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats_securev1_2017Q2.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats_securev1_2016Q1.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats_securev1_2016Q4.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats3c_securev1.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats_securev1_2017Q4.csv.zip\n",
      "Found 2 invalid rows which were removed\n",
      "    Reading file LoanStats3b_securev1.csv.zip\n",
      "Found 2 invalid rows which were removed\n"
     ]
    }
   ],
   "source": [
    "# Ingest the set of files we downloaded using the defined method \"ingest_files\"\n",
    "files_cs = ingest_files(dir_cs) # dictioary of (filename, dataframe) as (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cs = ... # combine \"files_cs\" into a pandas dataframe\n",
    "              # resent index with drop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep only the columns of interest from 'data_cs'\n",
    "final_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting with \" + str(len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typecast the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember that we read the data as string (without any formatting). \n",
    "# Now we would typecast the columns based on feature types which you found out in CS Phase 1\n",
    "\n",
    "for i in float_cols:\n",
    "    final_data[i] = ... # typecast float columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_perc(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return float(x.strip()[:-1])\n",
    "\n",
    "for i in perc_cols:\n",
    "    final_data[i] = ... # apply clean_perc to percentage columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_date(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.datetime.strptime( x, \"%b-%Y\").date()\n",
    "\n",
    "for i in date_cols:\n",
    "    final_data[i] = ... # typecast date cloumns to datatime using clean_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in cat_cols:\n",
    "    ... # for categorical features if the value is null/empty set it to None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate returns for each loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the names of the four returns we'll be calculating as described in Q.6\n",
    "# ret_PESS: Pessimistic return\n",
    "# ret_OPT: Optimistic return\n",
    "# ret_INTa, ret_INTb: Method3 at two differnt values of \"i\"\n",
    "ret_cols = [\"ret_PESS\", \"ret_OPT\", \"ret_INTa\", \"ret_INTb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all rows for loans that were paid back on the days they were issued\n",
    "final_data['loan_length'] = (final_data.last_pymnt_d - final_data.issue_d) / np.timedelta64(1, 'M')\n",
    "n_rows = len(final_data)\n",
    "\n",
    "final_data = ... # select rows where loan_length is not 0. \n",
    "\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1-Pessimistic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the return using a simple annualized profit margin\n",
    "# Pessimistic definition (Handout 6a.) (M1)\n",
    "\n",
    "final_data['term_num'] = final_data.term.str.extract('(\\d+)',expand=False).astype(int) # length of loan in months\n",
    "\n",
    "final_data['ret_PESS'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2-Optimistic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assuming that if a loan gives a positive return, we can\n",
    "# immediately find a similar loan to invest in; if the loan\n",
    "# takes a loss, we use M1-pessimistic to compute the return\n",
    "\n",
    "final_data['ret_OPT'] = ...\n",
    "\n",
    "final_data.loc[final_data.ret_OPT < 0,'ret_OPT'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ret_method_3(T, i):\n",
    "    '''\n",
    "    Given an investment time horizon (in months) and re-investment\n",
    "    interest rate, calculate the return of each loan\n",
    "    '''\n",
    "    \n",
    "    # Assuming that the total amount paid back was paid at equal\n",
    "    # intervals during the duration of the loan, calculate the\n",
    "    # size of each of these installment\n",
    "    actual_installment = (final_data.total_pymnt - final_data.recoveries) / ...\n",
    "\n",
    "    # Assuming the amount is immediately re-invested at the prime\n",
    "    # rate, find the total amount of money we'll have by the end\n",
    "    # of the loan\n",
    "    cash_by_end_of_loan = actual_installment * ... # compute the quantity given in [] in eq.2.3 of handout\n",
    "    \n",
    "    cash_by_end_of_loan = cash_by_end_of_loan + final_data.recoveries\n",
    "    \n",
    "    # Assuming that cash is then re-invested at the prime rate,\n",
    "    # with monthly re-investment, until T months from the start\n",
    "    # of the loan\n",
    "    remaining_months = T - final_data['loan_length']\n",
    "    final_return = cash_by_end_of_loan * ... \n",
    "\n",
    "    # Find the percentage return\n",
    "    ret_val = (12/T) * ...\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data['ret_INTa'] = ... # call ret_method_3 with T=60, i=0.002\n",
    "final_data['ret_INTb'] = ... # call ret_method_3 with T=60, i=0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_float_columns():\n",
    "    '''\n",
    "    This function visualizes Box-and-whisker plots for continuous variables\n",
    "    '''\n",
    "    \n",
    "    # FLoat columns\n",
    "    for i in float_cols + perc_cols + ret_cols:\n",
    "        seaborn.boxplot(final_data[i])\n",
    "\n",
    "        # Print the three highest values\n",
    "        highest_vals = ... # get 3 highest values\n",
    "        \n",
    "        smallest_val = min(final_data[i])\n",
    "        \n",
    "        plt.text(smallest_val, -0.3, highest_vals[0])\n",
    "        plt.text(smallest_val, -0.2, highest_vals[1])\n",
    "        plt.text(smallest_val, -0.1, highest_vals[2])\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_cat_columns():\n",
    "    '''\n",
    "    Lists the distinct values for categorical columns\n",
    "    '''\n",
    "    # Categorical columns \n",
    "    for i in cat_cols:\n",
    "        ... # print field name\n",
    "        ... # print number of distinct values\n",
    "        ... # for each distinct value print the number of occurances\n",
    "        print(\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_date_columns():\n",
    "    '''\n",
    "    This function visualizes a timeline density for dates\n",
    "    '''\n",
    "    \n",
    "    # Date columns\n",
    "    for i in date_cols:\n",
    "        final_data[final_data[i].isnull() == False][i].apply(lambda x : str(x.year) +\n",
    "                                                \"-\" + str(x.month)).value_counts(ascending = True).plot()\n",
    "        plt.title(i + \" (\" + str(final_data[i].isnull().sum()) + \" null values)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize continuous features\n",
    "...\n",
    "\n",
    "# visulaize categorical features\n",
    "...\n",
    "\n",
    "# visualize date columns\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are quite a few outliers. \n",
    "# Please identify top-k (decide this based on the visualization) features where outliers are most obvious\n",
    "n_rows = len(final_data)\n",
    "\n",
    "final_data = ... # remove outliers based 1st obvious feature\n",
    "final_data = ... # remove outliers based 2nd obvious feature\n",
    "...\n",
    "final_data = ... # remove outliers based kth obvious feature\n",
    "\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all loans that are still current\n",
    "n_rows = len(final_data)\n",
    "\n",
    "final_data = ...\n",
    "\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only include loans isssued since 2010\n",
    "n_rows = len(final_data)\n",
    "\n",
    "final_data = ...\n",
    "\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deal with null values. We allow cateogrical variables to be null\n",
    "# OTHER than grade, which is a particularly important categorical.\n",
    "# All non-categorical variables must be non-null, and we drop\n",
    "# rows that do not meet this requirement\n",
    "\n",
    "required_cols = set(cols_to_pick) - set(cat_cols) - set([\"id\"])\n",
    "required_cols.add(\"grade\")\n",
    "\n",
    "n_rows = len(final_data)\n",
    "\n",
    "... # drop rows that contain null based only on \"required_cols\"\n",
    "\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the data again after cleaning\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the feature correlations\n",
    "... # use sns scatter or pairplot\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize relation between loan status and features\n",
    "... # sns pairplot or scatter plot. Refer to recitations\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe after removing the outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "Solution to Q.7 from the handout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the percentage of loans by grade, the default by grade,\n",
    "# and the return of each grade\n",
    "perc_by_grade = (final_data.grade.value_counts()*100/len(final_data)).sort_index()\n",
    "\n",
    "default_by_grade = final_data.groupby(\"grade\").apply(lambda x : (x.loan_status != \"Fully Paid\").sum()*100/len(x) )\n",
    "ret_by_grade_OPT = ... # average return for M2-Optimistic for each loan grade\n",
    "ret_by_grade_PESS = ... # average return for M1-Pessimistic for each loan grade\n",
    "ret_by_grade_INTa = ... # average return for M3\n",
    "ret_by_grade_INTb = ... # average return for M3\n",
    "int_rate_by_grade = ... # average interest rate for each grade\n",
    "\n",
    "combined = pd.DataFrame(perc_by_grade)\n",
    "combined.columns = ['perc_of_loans']\n",
    "combined['perc_default'] = default_by_grade\n",
    "combined['avg_int_rate'] = int_rate_by_grade\n",
    "combined['return_OPT'] = ret_by_grade_OPT\n",
    "combined['return_PESS'] = ret_by_grade_PESS\n",
    "combined['return_INTa'] = ret_by_grade_INTa\n",
    "combined['return_INTb'] = ret_by_grade_INTb\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output of previous cell, write down your answers to Q.7 from the handout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the \"total_pymnt\" and \"recoveries\" from the list of continuous features\n",
    "continuous_features = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we remove `total_pymt` and `recoveries` from the data for the task of predicting whether to give loan or not, although these are highly predictive features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the prepared data for modeling in next Phase.\n",
    "pickle.dump( [final_data, discrete_features, continuous_features, ret_cols], open(pickle_file, \"wb\") )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
